{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "white-istanbul",
   "metadata": {},
   "source": [
    "# Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "metallic-israeli",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import requests\n",
    "\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "level-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    DL_ROOT = 'https://spamassassin.apache.org/old/publiccorpus/'\n",
    "\n",
    "    # get file names\n",
    "    r = requests.get(DL_ROOT)\n",
    "    pat = re.compile('<a href=[\\'\"]([\\w\\._]+)[\"\\']>([\\w\\._]+)</a>')\n",
    "    links = re.finditer(pat, r.text)\n",
    "    links = [link.group(1) for link in links]\n",
    "    README = links[-1]\n",
    "    links = links[:-1]\n",
    "\n",
    "    SPAM_PATH = os.path.join('datasets', 'spam')\n",
    "    os.makedirs(SPAM_PATH, exist_ok=True)\n",
    "\n",
    "    # fetch readme file\n",
    "    path = os.path.join(SPAM_PATH, 'readme.txt')\n",
    "    DL_URL = DL_ROOT + README\n",
    "    r = requests.get(DL_URL)\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "    # get spam files    \n",
    "    def fetch_data(file_name):\n",
    "        bz2_path = os.path.join(SPAM_PATH, file_name)\n",
    "        DL_URL = DL_ROOT + file_name\n",
    "        r = requests.get(DL_URL)\n",
    "        with open(bz2_path, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "            file_bz2 = tarfile.open(bz2_path)\n",
    "            file_bz2.extractall(path=SPAM_PATH)\n",
    "            file_bz2.close()\n",
    "\n",
    "    for link in links:\n",
    "        fetch_data(link)\n",
    "        \n",
    "    # clean up\n",
    "    for path in glob.glob(os.path.join(SPAM_PATH, '*.bz2')):\n",
    "        os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "supported-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "otherwise-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_readme():\n",
    "    file = os.path.join('datasets', 'spam', 'readme.txt')\n",
    "    with open(file, 'r') as f:\n",
    "        result = f.read()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "israeli-culture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pre>\n",
      "\n",
      "REVISION HISTORY OF THIS CORPUS:\n",
      "\n",
      "(**update**: Oct 21 2002 jm: added nearly 3000 more messages.)\n",
      "(**update**: Nov 24 2002 jm: removed Replied: and Forwarded: headers.)\n",
      "(**update**: Dec  4 2002 jm: removed a German message, some left-over\n",
      "SpamAssassin markup, and quite a few duplicate messages.  Also replaced header\n",
      "obfuscation using \"example.com\" with \"spamassassin.taint.org\", since\n",
      "example.com has no MX record.)\n",
      "(**update**: Feb 28 2003 jm: Bob Dickinson reported some leftover markup\n",
      "that should have been removed from the headers.  Now cleaned.)\n",
      "(**update**: Apr 23 2003 jm: removed 3 messages with malicious Javascript)\n",
      "(**update**: Oct 10 2003 jm: noted that we'd love to hear about papers ;)\n",
      "(**update**: Dec 16 2004 jm: changed a couple of hostnames in\n",
      "headers, in 20021010*/hard_ham/0198* and 20030228*/hard_ham/00230*.)\n",
      "(**update**: Mar  2 2005 jm: added note about live testing)\n",
      "(**update**: Mar 11 2005 jm: removed a listed-as-spam mail that was really\n",
      "a misclassified non-spam, namely '00529.0c8a07bb7b14576063ba0c1c4079e209'\n",
      "in 'spam_2'.)\n",
      "(**update**: Jan 31 2006 jm: added note about \"www.countermoon.com\")\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "***** IMPORTANT: Do Not Use These Mails For Testing a Live System ******\n",
      "\n",
      "Please note: do NOT send these emails into a live email system.   I've\n",
      "received several complaints from my correspondents that they've received\n",
      "bounce messages in response to mails in this corpus, due to misconfigured\n",
      "*LIVE* email systems being tested against this public corpus!\n",
      "\n",
      "I'm offering this as a service to spam filter developers, and causing\n",
      "trouble for my acquaintances and various mailing list administrators\n",
      "does NOT incline me to continue offering this data publically.\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Welcome to the SpamAssassin public mail corpus.  This is a selection of mail\n",
      "messages, suitable for use in testing spam filtering systems.  Pertinent\n",
      "points:\n",
      "\n",
      "  - All headers are reproduced in full.  Some address obfuscation has taken\n",
      "    place, and hostnames in some cases have been replaced with\n",
      "    \"spamassassin.taint.org\" (which has a valid MX record).  In most cases\n",
      "    though, the headers appear as they were received.\n",
      "\n",
      "  - All of these messages were posted to public fora, were sent to me in the\n",
      "    knowledge that they may be made public, were sent by me, or originated as\n",
      "    newsletters from public news web sites.\n",
      "\n",
      "  - relying on data from public networked blacklists like DNSBLs, Razor, DCC\n",
      "    or Pyzor for identification of these messages is not recommended, as a\n",
      "    previous downloader of this corpus might have reported them!\n",
      "\n",
      "  - Copyright for the text in the messages remains with the original senders.\n",
      "\n",
      "\n",
      "OK, now onto the corpus description.  It's split into three parts, as follows:\n",
      "\n",
      "  - spam: 500 spam messages, all received from non-spam-trap sources.\n",
      "\n",
      "  - easy_ham: 2500 non-spam messages.  These are typically quite easy to\n",
      "    differentiate from spam, since they frequently do not contain any spammish\n",
      "    signatures (like HTML etc).\n",
      "\n",
      "  - hard_ham: 250 non-spam messages which are closer in many respects to\n",
      "    typical spam: use of HTML, unusual HTML markup, coloured text,\n",
      "    \"spammish-sounding\" phrases etc.\n",
      "\n",
      "  - easy_ham_2: 1400 non-spam messages.  A more recent addition to the set.\n",
      "\n",
      "  - spam_2: 1397 spam messages.  Again, more recent.\n",
      "\n",
      "Total count: 6047 messages, with about a 31% spam ratio.\n",
      "\n",
      "The corpora are prefixed with the date they were assembled.  They are\n",
      "compressed using \"bzip2\".  The messages are named by a message number and\n",
      "their MD5 checksum.\n",
      "\n",
      "The \"obsolete\" dir contains old versions of the corpus, for reference,\n",
      "in case you need to correlate test results using these older versions\n",
      "against the source messages.  The messages in those corpora are generally\n",
      "included in the fresher corpora.\n",
      "\n",
      "This corpus lives at http://spamassassin.apache.org/publiccorpus/ .  Mail\n",
      "jm - public - corpus AT jmason dot org if you have questions.\n",
      "\n",
      "Note: if you write a paper or similar using this corpus, and it's available\n",
      "for download, we'd love to hear about it!  Mail users AT spamassassin dot\n",
      "apache dot org.  cheers!\n",
      "\n",
      "\n",
      "UPDATE: Jan 31 2006 jm: I've received a mail saying 'I'm seeing 41 messages\n",
      "[from the ham corpus] with the URL \"www.countermoon.com\" hit on SURBL.   Looks\n",
      "like the domain changed may have changed hands at some point.'    So again,\n",
      "live lookups will probably now produce different results from what would\n",
      "have been seen at time of first email receipt; be warned.\n",
      "\n",
      "</pre>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_readme())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-technology",
   "metadata": {},
   "source": [
    "# Creating a dataset\n",
    "\n",
    "- each directory in datasets/spam contains text files\n",
    "- extract these text files\n",
    "- need to figure out text encoding\n",
    "- choose which data to use (all?)\n",
    "- attach labels to the text files\n",
    "- separate a stratified test set\n",
    "- do we have enough data for hold-out validation, or should we use cross validation? ..if we want to use a hold-out set, we need to separate that as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "prescription-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_encoding(file_path):\n",
    "    file_info = subprocess.run(['file', '-i', file_path],\n",
    "                                capture_output=True)\n",
    "    file_info = file_info.stdout.decode('utf-8').split(' ')\n",
    "    encoding = file_info[-1].lstrip('charset=').rstrip('\\n')\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "precise-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email(dirs, encoding_='utf-8'):\n",
    "    email = []\n",
    "    error = []\n",
    "    for dir in dirs:\n",
    "        PATH = os.path.join(SPAM_PATH, dir)\n",
    "        dir_email = []\n",
    "        dir_error = []\n",
    "        for file in os.listdir(PATH):\n",
    "            file_path = os.path.join(PATH, file)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=encoding_) as f:\n",
    "                    dir_email.append(f.read())\n",
    "            except UnicodeError:\n",
    "                try:\n",
    "                    with open(file_path, 'r',\n",
    "                              encoding=get_encoding(file_path)) as f:\n",
    "                        dir_email.append(f.read())\n",
    "                except LookupError as e:\n",
    "                    dir_error.append('LookupError: {}\\tFile: {}'\n",
    "                                     .format(e, file_path))\n",
    "                except UnicodeError as e:\n",
    "                    try:\n",
    "                        with open(file_path, 'r',\n",
    "                                  encoding='windows-1253') as f:\n",
    "                            dir_email.append(f.read())\n",
    "                    except UnicodeError as e2:\n",
    "                        dir_error.append('UnicodeErr: {}{}\\tFile: {}'\n",
    "                                         .format(e, e2, file_path))\n",
    "        email.append(dir_email)\n",
    "        error.append(dir_error)\n",
    "\n",
    "    return list(zip(dirs, email)), error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ordinary-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emails(path=SPAM_PATH):\n",
    "    \"\"\"\n",
    "    Return two lists, ham_emails and spam_emails,\n",
    "    that contain pairs of the form (dir_name, list_of_emails),\n",
    "    where dir_name is the name of a directory, and list_of_emails\n",
    "    is a list of emails in that directory as strings.\n",
    "    \n",
    "    Note: 110 emails from the spam directories had unknown encodings\n",
    "    and they are not included in the output.\n",
    "    \"\"\"\n",
    "    SPAM_PATH = os.path.join('datasets', 'spam')\n",
    "\n",
    "    dirs = [dir for dir in os.listdir(SPAM_PATH)\n",
    "            if not dir.endswith('.txt')]\n",
    "    ham_dirs = [dir for dir in dirs if 'ham' in dir]\n",
    "    spam_dirs = [dir for dir in dirs if 'spam' in dir]\n",
    "    \n",
    "    ham_emails, ham_errors = get_email(ham_dirs, encoding_='windows-1252')\n",
    "    spam_emails, spam_errors = get_email(spam_dirs)\n",
    "    \n",
    "    return ham_emails, spam_emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-taste",
   "metadata": {},
   "source": [
    "## Train/val/test split\n",
    "\n",
    "Let's try 60/20/20; we need to stratify spam and ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "cubic-elimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "ham, spam = extract_emails()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "elder-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "israeli-advertiser",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test(ham, spam):\n",
    "    all_ham, all_spam = [], []\n",
    "    for x in ham:\n",
    "        all_ham.extend(x[1])\n",
    "    for x in spam:\n",
    "        all_spam.extend(x[1])\n",
    "        \n",
    "    random.shuffle(all_ham)\n",
    "    random.shuffle(all_spam)\n",
    "    \n",
    "    all_ham = [(x, 0) for x in all_ham]\n",
    "    all_spam = [(x, 1) for x in all_spam]\n",
    "    \n",
    "    n_ham = len(all_ham)\n",
    "    n_spam = len(all_spam)\n",
    "    \n",
    "    a, b = int(0.6 * n_ham), int(0.8 * n_ham)\n",
    "    c, d = int(0.6 * n_spam), int(0.8 * n_spam)\n",
    "    \n",
    "    train = all_ham[:a] + all_spam[:c]\n",
    "    val = all_ham[a:b] + all_spam[c:d]\n",
    "    test = all_ham[b:] + all_spam[d:]\n",
    "    \n",
    "    random.shuffle(train)\n",
    "    random.shuffle(val)\n",
    "    random.shuffle(test)\n",
    "    \n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "present-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = train_val_test(ham, spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "fancy-poker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5546 1849 1849\n"
     ]
    }
   ],
   "source": [
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "opposed-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(x):\n",
    "    y, z = zip(*x)\n",
    "    return list(y), list(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "mental-cherry",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = unzip(train)\n",
    "X_val, y_val = unzip(val)\n",
    "X_test, y_test = unzip(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-athens",
   "metadata": {},
   "source": [
    "# Processing emails\n",
    "\n",
    "- bag of words?\n",
    "- bigrams? n-grams?\n",
    "- header, address, salutation, signature features? (e.g. r'([-\\w]+): (.+)')\n",
    "- all lowercase, drop punctuation?\n",
    "- drop common words? (might lose salutation...)\n",
    "- use BeautifulSoup? \n",
    "- want to have different features as hyperparameters eventually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-reality",
   "metadata": {},
   "source": [
    "- Need a list of words to use... could combine output of all cleaning\n",
    "- remove very frequent or very infrequent words?\n",
    "- how can we remove \"non-word\" words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "clean-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "flush-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean1(email):\n",
    "    pat = re.compile(r'([-\\w]+: .+|<.*|.*>|.*NextPart.*|charset=.*|\\w+\\.\\w+\\.?\\w*\\.?\\w*\\.?\\w*)')\n",
    "    email2 = pat.sub(' ', email)\n",
    "    return [x.lower()\n",
    "            for x in re.findall(r'[A-Za-z]+', email2)\n",
    "            if len(x) > 2]\n",
    "\n",
    "def create_corpus(f, X):\n",
    "    \"X training set, f maps email to list of words\"\n",
    "    corpus = {}\n",
    "    for email in X:\n",
    "        for word in f(email):\n",
    "            corpus[word] = corpus.get(word, 0) + 1\n",
    "    return corpus\n",
    "\n",
    "def create_df(X, word_freq):\n",
    "    template = {k: 0 for k in sorted(word_freq.index.to_list())}\n",
    "    keys = template.keys()\n",
    "\n",
    "    def feature_vec1(email):\n",
    "        vec = template.copy()\n",
    "        for x in clean1(email):\n",
    "            if x in keys:\n",
    "                vec[x] = 1\n",
    "        return vec\n",
    "\n",
    "    df = pd.DataFrame({i: feature_vec1(email)\n",
    "                       for i, email in enumerate(X)})\n",
    "\n",
    "    return df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "cutting-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = pd.Series(create_corpus(clean1, X_train))\n",
    "filt = word_freq.between(6, 900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "fleet-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = create_df(X_train, word_freq[filt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "prostate-china",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5546, 12418)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "retained-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_df = create_df(X_val, word_freq[filt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "pharmaceutical-visit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1849, 13988)"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "whole-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_df = pd.Series(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "personalized-leisure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    1\n",
       "3    0\n",
       "4    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "changing-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_df = pd.Series(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-sleeve",
   "metadata": {},
   "source": [
    "# Training a quick model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-beaver",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "intended-release",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "quick-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "universal-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "aggregate-antigua",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train_df, y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "backed-mechanics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998196898665705"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train_df, y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "northern-affiliate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9929691725256896"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_val_df, y_val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "incomplete-redhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "little-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = clf.predict(X_train_df)\n",
    "y_val_preds = clf.predict(X_val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "sensitive-complement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4172,    0],\n",
       "       [  10, 1364]])"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train_df, y_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "defensive-masters",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1391,    0],\n",
       "       [  13,  445]])"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_val_df, y_val_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "industrial-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "indian-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(truth, preds):\n",
    "    print(f'Precision: {precision_score(truth, preds):.3f}')\n",
    "    print(f'Recall: {recall_score(truth, preds):.3f}')\n",
    "    print(f'F_1 score: {f1_score(truth, preds):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "opposite-subsection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.000\n",
      "Recall: 0.993\n",
      "F_1 score: 0.996\n"
     ]
    }
   ],
   "source": [
    "score(y_train_df, y_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "eleven-bacteria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.000\n",
      "Recall: 0.972\n",
      "F_1 score: 0.986\n"
     ]
    }
   ],
   "source": [
    "score(y_val_df, y_val_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-florist",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "widespread-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "assisted-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "promising-supply",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf.fit(X_train_df, y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "supposed-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_preds = nb_clf.predict(X_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "peaceful-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_val_preds = nb_clf.predict(X_val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "infrared-exhibit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.971\n",
      "Recall: 0.926\n",
      "F_1 score: 0.948\n"
     ]
    }
   ],
   "source": [
    "score(y_train_df, nb_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "australian-bargain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.979\n",
      "Recall: 0.913\n",
      "F_1 score: 0.945\n"
     ]
    }
   ],
   "source": [
    "score(y_val_df, nb_val_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-realtor",
   "metadata": {},
   "source": [
    "## Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "disciplinary-makeup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "native-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf = LinearSVC(max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "silver-district",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(max_iter=10000)"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_clf.fit(X_train_df, y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "central-stewart",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_train_preds = svc_clf.predict(X_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "flexible-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_val_preds = svc_clf.predict(X_val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "sixth-couple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.995\n",
      "Recall: 1.000\n",
      "F_1 score: 0.997\n"
     ]
    }
   ],
   "source": [
    "score(y_train_df, svc_train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "local-connectivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.000\n",
      "Recall: 0.972\n",
      "F_1 score: 0.986\n"
     ]
    }
   ],
   "source": [
    "score(y_val_df, y_val_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-eleven",
   "metadata": {},
   "source": [
    "# Improving predictions\n",
    "\n",
    "- set up pipeline to search over different word frequency thresholds\n",
    "- word frequency vectors vs. bag of words. (Need to rescale if we use word frequencies)\n",
    "- think of other features to add\n",
    "- think of other processing steps (stemming, tagging (e.g. each image, or each link)\n",
    "- tune regularization parameters\n",
    "- examine false negatives\n",
    "- probably don't need to look outside of linear models, since they're already performing quite well\n",
    "- it could just be coincidence, but Logit and SVC performed differently on the training set, so maybe we could ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-grass",
   "metadata": {},
   "source": [
    "## Making a transformer for our processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-amateur",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
